---
title: "Multiple predictors"
author: ""
date: "Duke University"
output:
  xaringan::moon_reader:
    mathjax: "https://cdn.bootcss.com/mathjax/2.7.1/MathJax.js?config=TeX-MML-AM_HTMLorMML"
    css: "sta199-slides.css"
    lib_dir: libs
    nature: 
      highlightLines: true
      highlightStyle: github
      countIncrementalSlides: false
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      dpi=300,
                      fig.height = 3,
                      fig.width = 5
)
```

```{r packages-data, echo=FALSE, message=FALSE, warning=FALSE}
library(tidyverse)
library(knitr)
library(broom)
beijing <- read_csv("data/beijing.csv")
```

class: center, middle

### Multivariable models

---

### Modeling

- We use models to explain relationships between variables and make 
predictions

- For now we will focus on **linear** models (but remember there are other types 
of models too!)

- Today, we will focus on fitting and interpreting models with a continuous 
outcome variable and **multiple** predictors. This type of model is often called a <font class = "vocab">multivariable</font> 
(*not multivariate*) model

- We will also explore assumptions of the linear model and assess them using
some additional functions from the `broom` package

---

class: center, middle

# Atmospheric measurements

---

### Today's data

```{r, out.width = "70%", echo = F, fig.align = "center"}
include_graphics("img/beijing1.png")
```
<center>
Series of atmospheric measurements taken at the
Agriculture Exhibition Hall in Beijing from 2013 - 2017
</center>
---

### `glimpse` the dataset

```{r, message = F, warning = F}
beijing <- read_csv("data/beijing.csv")
glimpse(beijing)
```

---

class: center, middle

### Modeling the relationship between variables

---

### Dew point and barometric pressure

```{r, message = F, warning = F, echo = F}
ggplot(data = beijing, aes(x = PRES, y = DEWP)) +
  geom_point() +
  geom_smooth(method = "lm", se = F) +
  ylim(c(-40, 30))
```

$$\widehat{DEWP} = 1071 - 1.08 PRES$$

---

### Can we do better?

We have a pretty good idea of what the dew point would be, **conditional** on
the value of the barometric pressure. But if we knew the temperature and  
precipitation, we might do even better!

Let's add some more predictors to our model:

```{r}
mod3 <- lm(DEWP ~ PRES + TEMP + RAIN, data = beijing)
tidy(mod3)
```

---

### The multiple regression model

The underlying linear model is given by

$$y_i = \beta_0 + \beta_1 x_{1i} + \beta_2x_{2i} + \cdots + \beta_px_{pi} + 
\epsilon_i$$

- $p$ is the total number of predictor or explanatory variables
- $y_i$ is the outcome variable for individual $i$
- $\beta_0$ is the intercept parameter
- $\beta_1, \beta_2, \cdots, \beta_p$ are the slope parameters
- $x_{1i}, \cdots, x_{pi}$ are the predictor variables
- $\epsilon_i$ is the unobserved error term

Interpretations for slope estimates are made **conditionally** on other 
predictors in the model. In this way, we can adjust for potential confounders
in our model.

---

### Interpreting coefficients

```{r, echo = F}
mod3 <- lm(DEWP ~ PRES + TEMP + RAIN, data = beijing)
tidy(mod3)
```

$$\widehat{DEWP} = 316 - 0.364 PRES + 0.723 TEMP + 1.15 RAIN$$

- If the barometric pressure, temperature, and precipitation are all 0, then we
would expect a dew point of 361 degrees Celsius (hmm...)

- For each additional hPa increase in barometric pressure, we would expect a
0.364 C *decrease* in dew point (on average), 
**holding temperature and precipiation constant**

- For each additional degree C increase in temperature, we would expect a 0.723
C increase in dew point (on average),
**holding barometric pressure and precipiation constant**

- For each additional mm increase in precipitation, we would expect a 1.15 C
increase in dew point (on average),
**holding barometric pressure and temperature constant**

---

### Inference with multiple predictors

```{r, echo = F}
tidy(mod3)
```

We may again test hypotheses that $\beta$ coefficients are equal to 0 (vs. the
alternative that they are not equal to 0). 

Be sure that any interpretation for slopes are made **conditionally** on the
other predictors in the model (that is, holding them constant/adjusting for 
them, etc.).

.question[
Is there sufficient evidence that barometric pressure has a linear association
with dew point, while adjusting for temperature and precipitation?
]

---

### Returning to $R^2$

```{r, echo = F}
glance(mod3)
```

We see that approximately 70% of the variability in dew point explained by its
linear relationship with barometric pressure, temperature, and dew point, which
is higher than the 60% explained by barometric pressure alone.

However, $R^2$ can never decrease when variables are added to a model, 
*even if they are useless for prediction*. We adjust for the number of 
predictors in our model to obtain **adjusted** $R^2$, where the adjustment 
is made to account for the number of predictors.

$R^2_{adj}$ incorporates a penalty for each additional predictor, so it will go 
down if a new variable does not meaningfully improve prediction

Note that $R^2_{adj}$ can **not** be interpreted as explained variability

---

### Interaction effects

Sometimes, the effect of one variable depends on the value of another. For 
example, the effect of exercise % on obesity may be different for smokers vs.
non-smokers. To model such a relationship, we create an <font class = "vocab">interaction term</font>.

We fit interaction terms in the model by multiplying predictors together:

```{r, echo = F}
mod4 <- lm(DEWP ~ PRES + TEMP + RAIN + TEMP*RAIN, data = beijing)
tidy(mod4)
```

\begin{align*}
\widehat{DEWP} &= 361 - 0.364 PRES + 0.725 TEMP + 9.01RAIN \\
&\mathrel{\phantom{=}} - 0.354 TEMP \times RAIN
\end{align*}
Interpretations become tricky once interactions are involved...

---

### Interpreting interaction effects

For a given PRES, TEMP, and RAIN, our predicted DEWP is given by
\begin{align*}
\widehat{DEWP} &= \hat{\beta}_0 + \hat{\beta}_1PRES + \hat{\beta}_2TEMP +\\
&\mathrel{\phantom{=}} \hat{\beta}_3RAIN + \hat{\beta}_4TEMP \times RAIN
\end{align*}

--

For a day with the same PRES AND TEMP but one higher unit of RAIN, the predicted
DEWP is given by
\begin{align*}
\widehat{DEWP}^\star &= \hat{\beta}_0 + \hat{\beta}_1PRES + \hat{\beta}_2TEMP +\\
&\mathrel{\phantom{=}} \hat{\beta}_3(RAIN + 1) + \hat{\beta}_4TEMP \times (RAIN + 1)
\end{align*}

--

Subtracting the two equations, we have
$$\widehat{DEWP} - \widehat{DEWP}^\star = \hat{\beta}_3 + \hat{\beta}_4 TEMP$$
The expected change in dew point for a one mm increase in precipitation, if 
pressure is held constant, is $\hat{\beta}_3 + \hat{\beta}_4 \times TEMP$

**The effect of precipitation on the dew point, adjusting for pressure, depends on temperature!**

---

class: center, middle

### Model assumptions

---

### Model assumptions

There are a few main assumptions to the linear model:

- The outcomes $y_i$ are <font class = "vocab">independent</font>. This means 
that observations do not "influence" each other in any way (e.g., repeated
outcome measures are made on the same individual)

- A <font class = "vocab">linear relationship</font> holds (though we can relax
this to some extent)

- The **error** terms are <font class = "vocab">normally distributed</font>, 
with mean zero and <font class = "vocab">constant variance</font> across all
predicted/fitted values

---

### `augment`ing model results

Note that many of the assumptions deal with the error term and fitted values,
which we might estimate using the residuals and predictions from our model.

We can use the `augment` function from the `broom` package to calculate these.
Let's consider the multiple regression model from earlier:

```{r}
augment(mod3) %>% 
  slice(1:5)
```

We will focus on the `.fitted` and `.resid` variables in this data frame

---

### `augment`ing model results

```{r}
augment(mod3) %>% 
  slice(1:5) %>% 
  select(.fitted, .resid)
```

- `.fitted` provides the predicted value of the response variable (for the given
observation)

- `.resid` provides the residual for the observation

---

### Evaluating independence

Unfortunately, in many cases we must simply think about how the data are 
collected. In this case, we have atmospheric measurements through time at a 
single station. Independence might **not** be satisfied -- we might imagine 
that if the dew point is high today, it'll probably still be high tomorrow.

---

### Evaluating independence

Note that the data are already sorted in 
chronological order. Let's plot the dew point through time, examining the first
three hundred data points:

```{r, fig.height = 2}
beijing %>% 
  slice(1:300) %>% 
  ggplot(data = ., mapping = aes(x = 1:300, y = DEWP)) + 
  geom_line() + 
  labs(x = "Measurement number", y = "Dew point (C)")
```

---

### Evaluating normality of errors

We can simply plot a histogram of the residuals and assess visually. There are
more formal methods, but for our purposes this will suffice. Note our use of the
`.resid` variable from the augmented model results.

```{r, message = F, fig.height = 2}
augment(mod3) %>% 
  ggplot(data = ., mapping = aes(x = .resid)) + 
  geom_histogram() +
  labs(x = "Residual", y = "Count",
       title = "Residuals are left-skewed")
```
We see that normality of the errors is likely **not** satisfied.

---

### The residual plot

One of the most useful diagnostic plots is the <font class = "vocab">residual plot</font>, 
which plots the fitted values against the residuals. Let's take a look:

```{r, fig.height = 2}
augment(mod3) %>% 
  ggplot(data = ., mapping = aes(x = .fitted, y = .resid)) + 
  geom_point() + 
  geom_hline(yintercept = 0, color = "red") + 
  labs(x = "Fitted value", y = "Residual")
```

---

### The residual plot

Ideally, the residual plot should be a "shapeless blob" with no real pattern. A
pattern would be indicative of potential violation of our model assumptions.

- If the residuals are not symmetric about the x-axis, then we might have
violation of linearity. This appears to be the case in our residual plot

- If the residuals do not have similar vertical spread for all fitted values,
then we might have violation of constant error variance (homoscedasticity of
errors). This might result in a "fanning" pattern. Our residual plot doesn't
suggest that this assumption violated.

---

class: center, middle

### Predicting values

---

### Making predictions

```{r, echo = F}
tidy(mod3)
```

Our fitted model is given by

$$\widehat{DEWP} = 360.7 - 0.36PRES + 0.72 TEMP + 1.15RAIN,$$

and can be used to predict dew points given the barometric pressure, temperature,
and precipitation.

.question[
Suppose the barometric pressure, temperature, and precipitation are equal to the
mean values in our dataset. What would the predicted dew point be? Is this close
to the mean dew point in our dataset?
]

---

### More `augment` fun

Let's revisit the `augment`ed model output.

```{r, echo = F}
augment(mod3)
```

Notice that the `augment` function provides fitted values and residuals (and 
more) for every observation in our dataset. Although we can plug in values to our model manually, this quickly becomes 
tedious.

We can use the `augment` function to create new predictions based on
existing models.

---

### Answering the question with `augment`

The mean pressure, temperature, and precipitation in our dataset are 1012.6 hPa, 
13.55 C, and 0.074 mm, respectively. Let's create a new `tibble` and "feed" it
into our `mod3` object:

```{r}
mean_vals  = tibble(PRES = 1012.6,
                    TEMP = 13.55,
                    RAIN = 0.074)
augment(mod3, newdata = mean_vals)
```

Note that the new dataset has the same variable names as found in our model.

We predict a dew point of 2.38 C and note that the standard error for this
prediction is 0.195 C.

.question[
How might we use these results to create interval estimates?
]